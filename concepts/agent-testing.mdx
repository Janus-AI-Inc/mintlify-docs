---
title: "Agent Testing"
description: "Learn how Janus automatically tests your AI agents through conversation simulations"
---

## Automated Agent Testing

Janus Python SDK provides comprehensive automated testing for AI agents through conversation simulations. Instead of manually creating test scenarios, Janus automatically generates diverse conversation patterns to stress-test your agent.

## How It Works

### 1. **Conversation Simulation**
Janus creates realistic conversation scenarios by simulating different user personas interacting with your agent:

```python
await janus.run_simulations(
    num_simulations=10,  # Run 10 different conversations
    max_turns=5,         # Each conversation has up to 5 turns
    target_agent=lambda: MyAgent().chat,
    api_key="your_janus_api_key"
)
```

### 2. **Diverse Personas**
Each simulation uses different personas with varying characteristics:
- **Demographics**: Age, income, gender, location
- **Behavioral traits**: Conversation intensity, patience, complaint propensity
- **Technical expertise**: Beginner to expert levels
- **Communication styles**: Formal, casual, urgent, relaxed

### 3. **Systematic Testing**
Janus systematically tests your agent across multiple dimensions:

<CardGroup cols={2}>
<Card title="Response Quality" icon="star">
  Evaluates helpfulness, accuracy, and relevance of responses
</Card>

<Card title="Policy Compliance" icon="shield">
  Checks adherence to safety guidelines and business rules
</Card>

<Card title="Context Awareness" icon="brain">
  Tests ability to maintain conversation context
</Card>

<Card title="Error Handling" icon="alert-triangle">
  Identifies how agent handles edge cases and errors
</Card>
</CardGroup>

## Testing Workflow

### Step 1: Define Your Agent
Create an agent class with a `chat` method that Janus can test:

```python
class MyAgent:
    def __init__(self):
        self.client = AsyncOpenAI(api_key=os.getenv("OPENAI_API_KEY"))
    
    async def chat(self, prompt: str) -> str:
        """This method will be tested by Janus"""
        response = await self.client.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": prompt}]
        )
        return response.choices[0].message.content
```

### Step 2: Configure Test Parameters
Set up your testing configuration:

```python
# Basic testing
results = await janus.run_simulations(
    num_simulations=20,
    max_turns=8,
    target_agent=lambda: MyAgent().chat,
    api_key="your_janus_api_key"
)

# Advanced testing with custom context
results = await janus.run_simulations(
    num_simulations=50,
    max_turns=10,
    target_agent=lambda: MyAgent().chat,
    api_key="your_janus_api_key",
    context="You are a customer service representative testing a new AI assistant.",
    goal="Evaluate the AI assistant's ability to handle customer inquiries professionally."
)
```

### Step 3: Analyze Results
Each simulation provides detailed results:

<ResponseField name="conversation" type="List[dict]" required>
Full conversation history with timestamps and participant information
</ResponseField>

<ResponseField name="metrics" type="dict" optional>
Performance metrics including response times and token usage
</ResponseField>

<ResponseField name="evaluation" type="dict" optional>
Rule-based evaluation results and judgment scores
</ResponseField>

## Integration with Janus Platform

The SDK works seamlessly with the Janus web platform:

### **Frontend Configuration**
Through the Janus dashboard (`/dashboard/chat`), you can:

1. **Create Chat Templates**: Design specific persona populations
   - Set demographic parameters (age, income, gender, location)
   - Configure behavioral traits (conversation intensity, patience)
   - Define urgency levels and communication styles

2. **Configure Test Cases**: Create structured question sets
   - Define test case content and context
   - Upload evaluation datasets for comparison testing

3. **Set Up Rule Sets**: Define evaluation criteria
   - Write individual rules for each conversation turn
   - Define overall conversation rules
   - Use pre-built rule templates

### **Results Analysis**
View comprehensive results in the dashboard at [app.withjanus.com](https://app.withjanus.com):

- **Conversation Transcripts**: Full conversation flow with timestamps
- **Rule Violations**: Specific rules violated in each turn
- **RAG Hallucinations**: Claims that couldn't be verified against knowledge base
- **Investigation Results**: Autonomous judging and usefulness assessments
- **Performance Traces**: Technical execution details and metrics

## Best Practices

<Tip>
- Start with 10-20 simulations for initial testing
- Increase simulation count for comprehensive evaluation
- Use custom context to match your specific use case
- Monitor results in the Janus dashboard for detailed insights
</Tip>

<Warning>
Running many simulations can be resource-intensive. Monitor your API usage and system resources.
</Warning>

## Example: Customer Service Testing

```python
# Test a customer service agent
results = await janus.run_simulations(
    num_simulations=30,
    max_turns=6,
    target_agent=lambda: CustomerServiceAgent().chat,
    api_key="your_janus_api_key",
    context="You are a customer testing a new AI customer service assistant.",
    goal="Test the AI assistant's ability to handle customer inquiries and resolve issues.",
    rules=[
        "The agent should be polite and professional",
        "The agent should provide accurate information",
        "The agent should escalate complex issues appropriately"
    ]
)
```

## Next Steps

<CardGroup cols={2}>
<Card title="Function Tracing" icon="route" href="/concepts/tracing">
  Learn how to trace function calls and tool usage
</Card>

<Card title="Evaluation Rules" icon="target" href="/concepts/evaluation">
  Understand how rule-based evaluation works
</Card>

<Card title="Quickstart Guide" icon="rocket" href="/quickstart">
  Run your first agent test
</Card>

<Card title="API Reference" icon="code" href="/api-reference/run-simulations">
  Detailed API documentation
</Card>
</CardGroup> 